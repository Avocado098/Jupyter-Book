{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668969fc-6f63-4b2b-b76e-cf5eaa2f91ff",
   "metadata": {},
   "source": [
    "Name: **Rachel Jasmine Canaman** <br>\n",
    "Section: **DS4A**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615c3a2-7742-4218-8d2b-608a84d6e4fb",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>Laboratory Task 3</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70641e50-eb0f-45e1-8ebc-a125fd164585",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"image2.png\" alt=\"A centered image\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631c3eb-1eec-4a75-bcbd-71a33f70d902",
   "metadata": {},
   "source": [
    "**Objective:**  \n",
    "Perform a forward and backward propagation in Python using the inputs from Laboratory Task 2.\n",
    "\n",
    "We will: <br>\n",
    "-Perform a forward pass (already done in Task 2). <br>\n",
    "-Compute the error and gradients via backward propagation. <br>\n",
    "-Update weights using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89fa0a5-cad1-448b-97fa-dcd164ef6318",
   "metadata": {},
   "source": [
    "**Step 1: Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eda859c-f90f-4eb5-a73e-3fdee9dff29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69136f-0430-4ad6-bd1e-c3a1e1c42d5a",
   "metadata": {},
   "source": [
    "**Step 2: Initialize Data, Weights, and Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ace5d8d-c5e6-4cac-8cca-c625e67f99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and target\n",
    "x = np.array([1, 0, 1])   # input vector\n",
    "y = np.array([1])         # target\n",
    "\n",
    "# Hidden layer weights\n",
    "W_hidden = np.array([\n",
    "    [0.2, -0.3],\n",
    "    [0.4,  0.1],\n",
    "    [-0.5, 0.2]\n",
    "])\n",
    "\n",
    "# Biases\n",
    "theta1, theta2, theta3 = -0.4, 0.2, 0.1\n",
    "\n",
    "# Output weights\n",
    "w_out = np.array([-0.3, -0.2])\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e7039-9f5b-46a3-9d97-edfa9fef2b65",
   "metadata": {},
   "source": [
    "**Step 3: Define Helper Functions** <br>\n",
    "We use ReLU as activation and its derivative for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d9210a-d094-4283-8b6a-6867a5d4c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35663877-d2c4-4c37-a988-8fa93d70471c",
   "metadata": {},
   "source": [
    "**Step 4: Forward Pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359d9b32-236b-46dc-b3db-6fa458a3cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden pre-activations: [-0.7  0.1]\n",
      "Hidden activations: [0.  0.1]\n",
      "Output pre-activation: 0.08\n",
      "Predicted output: 0.08\n",
      "Error: [0.4232]\n"
     ]
    }
   ],
   "source": [
    "# Hidden layer forward pass\n",
    "z_hidden = x @ W_hidden + np.array([theta1, theta2])\n",
    "h = relu(z_hidden)\n",
    "\n",
    "# Output layer forward pass\n",
    "z_out = h @ w_out + theta3\n",
    "y_hat = relu(z_out)\n",
    "\n",
    "# Compute error (MSE with 1/2 factor)\n",
    "error = 0.5 * (y - y_hat)**2\n",
    "\n",
    "print(\"Hidden pre-activations:\", z_hidden)\n",
    "print(\"Hidden activations:\", h)\n",
    "print(\"Output pre-activation:\", z_out)\n",
    "print(\"Predicted output:\", y_hat)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b285b-a51e-4148-a428-9b2eea0ee948",
   "metadata": {},
   "source": [
    "**Step 5: Backward Pass** <br>\n",
    "We compute gradients using the chain rule:\n",
    "\n",
    "1. $\\frac{\\partial E}{\\partial \\hat{y}} = \\hat{y} - y$\n",
    "2. Propagate through output layer weights and ReLU derivative.  \n",
    "3. Propagate through hidden layer weights and ReLU derivative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93162838-a11c-4ef8-92ee-c97fa90b4bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for output weights: [-0.    -0.092]\n",
      "Gradient for output bias: [-0.92]\n",
      "Gradients for hidden weights:\n",
      " [[0.    0.184]\n",
      " [0.    0.   ]\n",
      " [0.    0.184]]\n",
      "Gradients for hidden biases: [0.    0.184]\n"
     ]
    }
   ],
   "source": [
    "# Gradient of error wrt prediction\n",
    "dE_dyhat = y_hat - y  \n",
    "\n",
    "# Gradient at output pre-activation\n",
    "dyhat_dzout = relu_derivative(z_out)  \n",
    "dE_dzout = dE_dyhat * dyhat_dzout  \n",
    "\n",
    "# Gradients for output weights and bias\n",
    "dE_dw_out = h * dE_dzout\n",
    "dE_dtheta3 = dE_dzout\n",
    "\n",
    "# Backprop to hidden layer\n",
    "dE_dh = w_out * dE_dzout\n",
    "dh_dz = relu_derivative(z_hidden)\n",
    "dE_dz_hidden = dE_dh * dh_dz\n",
    "\n",
    "# Gradients for hidden weights and biases\n",
    "dE_dW_hidden = np.outer(x, dE_dz_hidden)\n",
    "dE_dtheta_hidden = dE_dz_hidden\n",
    "\n",
    "print(\"Gradients for output weights:\", dE_dw_out)\n",
    "print(\"Gradient for output bias:\", dE_dtheta3)\n",
    "print(\"Gradients for hidden weights:\\n\", dE_dW_hidden)\n",
    "print(\"Gradients for hidden biases:\", dE_dtheta_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac0b53-7c7f-4867-b763-15b176bd2ccd",
   "metadata": {},
   "source": [
    "**Step 6: Update Weights** <br>\n",
    "\n",
    "We apply gradient descent: $w \\leftarrow w - \\eta \\cdot \\frac{\\partial E}{\\partial w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8567462c-85d1-486d-aa52-12cf0ff86f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated hidden weights:\n",
      " [[ 0.2      -0.300184]\n",
      " [ 0.4       0.1     ]\n",
      " [-0.5       0.199816]]\n",
      "Updated hidden biases: [np.float64(-0.4), np.float64(0.19981600000000002)]\n",
      "Updated output weights: [-0.3      -0.199908]\n",
      "Updated output bias: [0.10092]\n"
     ]
    }
   ],
   "source": [
    "# Update output weights and bias\n",
    "w_out -= lr * dE_dw_out\n",
    "theta3 -= lr * dE_dtheta3\n",
    "\n",
    "# Update hidden weights and biases\n",
    "W_hidden -= lr * dE_dW_hidden\n",
    "theta1 -= lr * dE_dtheta_hidden[0]\n",
    "theta2 -= lr * dE_dtheta_hidden[1]\n",
    "\n",
    "print(\"Updated hidden weights:\\n\", W_hidden)\n",
    "print(\"Updated hidden biases:\", [theta1, theta2])\n",
    "print(\"Updated output weights:\", w_out)\n",
    "print(\"Updated output bias:\", theta3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0de10-0afe-4003-ab9a-2fb2bf3655b2",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- The forward pass produced a prediction of  $\\hat{y} \\approx 0.08$. \n",
    "- The error was relatively high $E \\approx 0.423$, consistent with Task 2.  \n",
    "- Backward propagation computed gradients for all weights and biases.  \n",
    "- A small learning rate $\\eta = 0.001$ was applied to update parameters.  \n",
    "\n",
    "This exercise demonstrates the mechanics of backpropagation: errors flow backward from the output to the hidden layer, adjusting weights to reduce prediction error. We have successfully implemented both forward and backward propagation. This process underpins all neural network training, enabling learning from data through error minimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
